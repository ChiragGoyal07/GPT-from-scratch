{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tiny Transformer Components in NumPy\n",
        "This notebook implements the following components using Python and NumPy:\n",
        "1. Token Embedding (Lookup table)\n",
        "2. Scaled Dot-Product Attention (with Causal Mask)\n",
        "3. Layer Normalization\n",
        "4. Dropout\n",
        "5. Feed-Forward Neural Network\n",
        "6. Skip Connections\n",
        "7. Output Layer (Vocab Projection)\n",
        "8. Cross-Entropy Loss\n",
        "9. Text Generation Strategies: Temperature, Top-k, Top-p Sampling\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1. Token Embedding\n",
        "vocab = {0: '<PAD>', 1: 'hello', 2: 'world', 3: 'good', 4: 'morning'}\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 4\n",
        "embedding_matrix = np.random.randn(vocab_size, embedding_dim)\n",
        "\n",
        "def tokens_to_embeddings(tokens):\n",
        "    return embedding_matrix[tokens]\n",
        "\n",
        "def embeddings_to_tokens(embeddings):\n",
        "    logits = embeddings @ embedding_matrix.T\n",
        "    return np.argmax(logits, axis=-1)\n",
        "\n",
        "# Test\n",
        "input_tokens = np.array([1, 3, 4])\n",
        "print('Embeddings:', tokens_to_embeddings(input_tokens))"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2. Scaled Dot-Product Attention with Causal Mask\n",
        "def softmax(x):\n",
        "    exp = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return exp / np.sum(exp, axis=-1, keepdims=True)\n",
        "\n",
        "def causal_mask(size):\n",
        "    return np.tril(np.ones((size, size)))\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    d_k = Q.shape[-1]\n",
        "    scores = Q @ K.T / np.sqrt(d_k)\n",
        "    mask = causal_mask(Q.shape[0])\n",
        "    scores = np.where(mask == 1, scores, -np.inf)\n",
        "    weights = softmax(scores)\n",
        "    return weights @ V\n",
        "\n",
        "# Test\n",
        "x = tokens_to_embeddings(input_tokens)\n",
        "Q = K = V = x\n",
        "print('Attention Output:', scaled_dot_product_attention(Q, K, V))"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 3. Layer Normalization\n",
        "def layer_norm(x, eps=1e-6):\n",
        "    mean = np.mean(x, axis=-1, keepdims=True)\n",
        "    std = np.std(x, axis=-1, keepdims=True)\n",
        "    return (x - mean) / (std + eps)\n",
        "\n",
        "# Test\n",
        "print('LayerNorm:', layer_norm(x))"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 4. Dropout\n",
        "def dropout(x, p=0.1):\n",
        "    mask = (np.random.rand(*x.shape) > p).astype(np.float32)\n",
        "    return x * mask / (1 - p)\n",
        "\n",
        "# Test\n",
        "print('Dropout:', dropout(x))"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 5. Feed-Forward Neural Network\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def feed_forward(x, W1, b1, W2, b2):\n",
        "    return relu(x @ W1 + b1) @ W2 + b2\n",
        "\n",
        "# Initialize params\n",
        "W1 = np.random.randn(embedding_dim, 8)\n",
        "b1 = np.random.randn(8)\n",
        "W2 = np.random.randn(8, embedding_dim)\n",
        "b2 = np.random.randn(embedding_dim)\n",
        "\n",
        "# Test\n",
        "print('FFN:', feed_forward(x, W1, b1, W2, b2))"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 6. Transformer Block with Skip Connections\n",
        "def transformer_block(x, W_q, W_k, W_v, W1, b1, W2, b2):\n",
        "    Q, K, V = x @ W_q, x @ W_k, x @ W_v\n",
        "    attn = scaled_dot_product_attention(Q, K, V)\n",
        "    x = layer_norm(x + attn)\n",
        "    ffn = feed_forward(x, W1, b1, W2, b2)\n",
        "    x = layer_norm(x + ffn)\n",
        "    return x\n",
        "\n",
        "# Initialize weights for block\n",
        "W_q = np.random.randn(embedding_dim, embedding_dim)\n",
        "W_k = np.random.randn(embedding_dim, embedding_dim)\n",
        "W_v = np.random.randn(embedding_dim, embedding_dim)\n",
        "\n",
        "# Test\n",
        "print('Transformer Block:', transformer_block(x, W_q, W_k, W_v, W1, b1, W2, b2))"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 7. Output Layer\n",
        "def output_layer(x, W_out, b_out):\n",
        "    return x @ W_out + b_out\n",
        "\n",
        "# Initialize\n",
        "W_out = np.random.randn(embedding_dim, vocab_size)\n",
        "b_out = np.random.randn(vocab_size)\n",
        "\n",
        "# Test\n",
        "print('Logits:', output_layer(x, W_out, b_out))"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 8. Cross-Entropy Loss\n",
        "def cross_entropy_loss(logits, targets):\n",
        "    probs = softmax(logits)\n",
        "    loss = -np.log(probs[np.arange(len(targets)), targets])\n",
        "    return np.mean(loss)\n",
        "\n",
        "# Test\n",
        "targets = np.array([3, 4, 2])\n",
        "print('Loss:', cross_entropy_loss(output_layer(x, W_out, b_out), targets))"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 9. Text Generation Strategies\n",
        "def temperature_sampling(logits, T=1.0):\n",
        "    scaled = logits / T\n",
        "    return np.random.choice(len(logits), p=softmax(scaled))\n",
        "\n",
        "def top_k_sampling(logits, k=2):\n",
        "    idx = np.argsort(logits)[-k:]\n",
        "    probs = softmax(logits[idx])\n",
        "    return np.random.choice(idx, p=probs)\n",
        "\n",
        "def top_p_sampling(logits, p=0.9):\n",
        "    sorted_idx = np.argsort(logits)[::-1]\n",
        "    sorted_probs = softmax(logits[sorted_idx])\n",
        "    cum_probs = np.cumsum(sorted_probs)\n",
        "    cutoff = np.searchsorted(cum_probs, p)\n",
        "    selected = sorted_idx[:cutoff+1]\n",
        "    probs = softmax(logits[selected])\n",
        "    return np.random.choice(selected, p=probs)\n",
        "\n",
        "# Example\n",
        "logits = output_layer(x, W_out, b_out)\n",
        "print('Temp sample:', temperature_sampling(logits[0]))  # sample token"
      ],
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}